{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hillClimbingAlgorithm.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMHN28z4G2araGQT0tUiVPl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aksevenli/Pytorch-Reinforcement-Learning/blob/master/hillClimbingAlgorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmFt1Pony_0b",
        "colab_type": "text"
      },
      "source": [
        "## Developing the hill-climbing algorithm\n",
        "As we can see in the random search policy, each episode is independent. In fact, all episodes in random search can be run in parallel, and the weight that achieves the best performance will eventually be selected. We've also verified this with the plot of reward versus episode, where there is no upward trend. In this recipe, we will develop a different algorithm, a hill-climbing algorithm, to transfer the knowledge acquired in one episode to the next episode.\n",
        "\n",
        "In the hill-climbing algorithm, we also start with a randomly chosen weight. But here, **for every episode, we add some noise to the weight**. **If the total reward improves, we update the weight with the new one**; otherwise, we keep the old weight. In this approach, the weight is gradually improved as we progress through the episodes, instead of jumping around in each episode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0rqa-RgvFj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDAIlZ_Zy-z_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "n_state = env.observation_space.shape[0]\n",
        "n_action = env.action_space.n\n",
        "\n",
        "def run_episode(env, weight):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    is_done = False\n",
        "\n",
        "    while not is_done:\n",
        "        state = torch.from_numpy(state).float()\n",
        "        action = torch.argmax(torch.matmul(state, weight))\n",
        "        state, reward, is_done, _ = env.step(action.item())\n",
        "        total_reward += reward\n",
        "    return total_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-ZjcX-vzy4u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "b4582e3d-dec4-4552-a4a7-83aa27cce65a"
      },
      "source": [
        "n_episode = 1000\n",
        "best_total_reward = 0\n",
        "best_weight = torch.rand(n_state, n_action)\n",
        "total_rewards = []\n",
        "\n",
        "# Add some noise to the weight for each episode\n",
        "# Apply a scale to the noise so that the noise won't overwhelm the weight\n",
        "noise_scale = 0.01"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3abc3f4eded6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_episode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbest_total_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbest_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtotal_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjg2zdtY2L7P",
        "colab_type": "text"
      },
      "source": [
        "After we randomly pick an initial weight, for each episode, we do the following:\n",
        "\n",
        "\n",
        "*   Add random noise to the weight\n",
        "*   Let the agent take actions according to the linear mapping\n",
        "*   An episode terminates and returns the total reward\n",
        "*   If the current reward is greater than the best one obtained so far, update the best reward and the weight\n",
        "*   Otherwise, the best reward and the weight remain unchanged\n",
        "*   Also, keep a record of the total reward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrdRQuQg3Fw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for episode in range(n_episode):\n",
        "    weight = best_weight + noise_scale * torch.rand(n_state, n_action)\n",
        "    total_reward = run_episode(env, weight)\n",
        "    if total_reward >= best_total_reward:\n",
        "        best_weight = best_weight\n",
        "    total_rewards.append(total_reward)\n",
        "    print('Episode {}: {}'.format(episode + 1, total_reward))\n",
        "print('Average total reward over {} episode: {}'.format(n_episode, sum(total_rewards) / n_episode))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_vLDhse4bgc",
        "colab_type": "text"
      },
      "source": [
        "To assess the training using the hill-climbing algorithm, we repeat the training process multiple times. We observe that the average total reward fluctuates a lot. What could cause such variance? **It turns out that if the initial weight is bad, adding noise at a small scale will have little effect on improving the performance**. This will cause **poor convergence**. On the other hand, if the initial weight is good, adding noise at a big scale might move the weight away from the optimal weight and jeopardize the performance. How can we make the training of the hillclimbing model more stable and reliable? We can actually make the noise scale adaptive to the performance, just like the adaptive learning rate in gradient descent.\n",
        "To make the noise adaptive, we do the following:\n",
        "\n",
        "\n",
        "*   Specify a starting noise scale.\n",
        "*   If the performance in an episode **improves**, **decrease** the noise scale. In our case, we take half of the scale, but set **0.0001** as the lower bound.\n",
        "*   If the performance in an episode **drops**, **increase** the noise scale. In our case, we double the scale, but set **2** as the upper bound.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wT9HZlj4Rks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "noise_scale = 0.01\n",
        "best_total_reward = 0\n",
        "total_rewards = []\n",
        "for episode in range(n_episode):\n",
        "    weight = best_weight + noise_scale * torch.rand(n_state, n_action)\n",
        "    total_reward = run_episode(env, weight)\n",
        "    if total_reward >= best_total_reward:\n",
        "        best_total_reward = total_reward\n",
        "        best_weight = weight\n",
        "        noise_scale = max(noise_scale / 2, 1e-4)\n",
        "    else:\n",
        "        noise_scale = min(noise_scale * 2, 2)\n",
        "    print('Episode {}: {}'.format(episode + 1, total_reward))\n",
        "    total_rewards.append(total_reward)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd76JZq8_jx1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57d2e455-ad4e-4e5d-c161-c3bdbf268a6b"
      },
      "source": [
        "print('Average total reward over {} episode: {}'.format(n_episode, sum(total_rewards) / n_episode))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average total reward over 1000 episode: 196.679\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6Qa657Z_wKv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "e67fefdf-60d8-439d-d345-5974cc7d9dba"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(total_rewards)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYRMo__1_84Q",
        "colab_type": "text"
      },
      "source": [
        "In the resulting plot, we can see a clear upward trend before it plateaus at the maximum value\n",
        "\n",
        "Now, let's see how the learned policy performs on 100 new episodes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCDuk7ZK_3fW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "90414001-219c-4422-91f9-7a2383f3d170"
      },
      "source": [
        "n_episode_eval = 100\n",
        "total_rewards_eval = []\n",
        "for episode in range(n_episode_eval):\n",
        "    total_reward = run_episode(env, best_weight)\n",
        "    print('Episode {}: {}'.format(episode+1, total_reward))\n",
        "    total_rewards_eval.append(total_reward)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 1: 200.0\n",
            "Episode 2: 200.0\n",
            "Episode 3: 200.0\n",
            "Episode 4: 200.0\n",
            "Episode 5: 200.0\n",
            "Episode 6: 200.0\n",
            "Episode 7: 200.0\n",
            "Episode 8: 200.0\n",
            "Episode 9: 200.0\n",
            "Episode 10: 200.0\n",
            "Episode 11: 200.0\n",
            "Episode 12: 200.0\n",
            "Episode 13: 200.0\n",
            "Episode 14: 200.0\n",
            "Episode 15: 200.0\n",
            "Episode 16: 200.0\n",
            "Episode 17: 200.0\n",
            "Episode 18: 200.0\n",
            "Episode 19: 200.0\n",
            "Episode 20: 200.0\n",
            "Episode 21: 200.0\n",
            "Episode 22: 200.0\n",
            "Episode 23: 200.0\n",
            "Episode 24: 200.0\n",
            "Episode 25: 200.0\n",
            "Episode 26: 200.0\n",
            "Episode 27: 200.0\n",
            "Episode 28: 200.0\n",
            "Episode 29: 200.0\n",
            "Episode 30: 200.0\n",
            "Episode 31: 200.0\n",
            "Episode 32: 200.0\n",
            "Episode 33: 200.0\n",
            "Episode 34: 200.0\n",
            "Episode 35: 200.0\n",
            "Episode 36: 200.0\n",
            "Episode 37: 200.0\n",
            "Episode 38: 200.0\n",
            "Episode 39: 200.0\n",
            "Episode 40: 200.0\n",
            "Episode 41: 200.0\n",
            "Episode 42: 200.0\n",
            "Episode 43: 200.0\n",
            "Episode 44: 200.0\n",
            "Episode 45: 200.0\n",
            "Episode 46: 200.0\n",
            "Episode 47: 200.0\n",
            "Episode 48: 200.0\n",
            "Episode 49: 200.0\n",
            "Episode 50: 200.0\n",
            "Episode 51: 200.0\n",
            "Episode 52: 200.0\n",
            "Episode 53: 200.0\n",
            "Episode 54: 200.0\n",
            "Episode 55: 200.0\n",
            "Episode 56: 200.0\n",
            "Episode 57: 200.0\n",
            "Episode 58: 200.0\n",
            "Episode 59: 200.0\n",
            "Episode 60: 200.0\n",
            "Episode 61: 200.0\n",
            "Episode 62: 200.0\n",
            "Episode 63: 200.0\n",
            "Episode 64: 200.0\n",
            "Episode 65: 200.0\n",
            "Episode 66: 200.0\n",
            "Episode 67: 200.0\n",
            "Episode 68: 200.0\n",
            "Episode 69: 200.0\n",
            "Episode 70: 200.0\n",
            "Episode 71: 200.0\n",
            "Episode 72: 200.0\n",
            "Episode 73: 200.0\n",
            "Episode 74: 200.0\n",
            "Episode 75: 200.0\n",
            "Episode 76: 200.0\n",
            "Episode 77: 200.0\n",
            "Episode 78: 200.0\n",
            "Episode 79: 200.0\n",
            "Episode 80: 200.0\n",
            "Episode 81: 200.0\n",
            "Episode 82: 200.0\n",
            "Episode 83: 200.0\n",
            "Episode 84: 200.0\n",
            "Episode 85: 200.0\n",
            "Episode 86: 200.0\n",
            "Episode 87: 200.0\n",
            "Episode 88: 200.0\n",
            "Episode 89: 200.0\n",
            "Episode 90: 200.0\n",
            "Episode 91: 200.0\n",
            "Episode 92: 200.0\n",
            "Episode 93: 200.0\n",
            "Episode 94: 200.0\n",
            "Episode 95: 200.0\n",
            "Episode 96: 200.0\n",
            "Episode 97: 200.0\n",
            "Episode 98: 200.0\n",
            "Episode 99: 200.0\n",
            "Episode 100: 200.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4WRSpLjALcp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5fcbd352-e339-4497-cedb-f13ba0136209"
      },
      "source": [
        "print('Average total reward over {} episode: {}'.format(n_episode, sum(total_rewards) / n_episode))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average total reward over 1000 episode: 196.679\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3Rfpf1eARLb",
        "colab_type": "text"
      },
      "source": [
        "The average reward for the testing episodes is close to the maximum of 200 that we obtained with the learned policy. You can re-run the evaluation multiple times. The results are pretty consistent.\n",
        "\n",
        "We are able to achieve much better performance with the hill-climbing algorithm than with random search by simply **adding adaptive noise** to each episode. We can think of it as a special kind of gradient descent without a target variable. The additional noise is the gradient, albeit in a random way. **The noise scale is the learning rate**, and it is adaptive to the reward from the previous episode. The target variable in hill climbing becomes achieving the highest reward. In summary, rather than isolating each episode, the agent in\n",
        "the hill-climbing algorithm makes use of the knowledge learned from each episode and performs a more reliable action in the next episode. As the name hill climbing implies, the reward moves upwards through the episodes as the weight gradually moves towards the optimum value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Mzm3rPJfoQC",
        "colab_type": "text"
      },
      "source": [
        "We can observe that the reward can reach the maximum value within the first 100 episodes. Can we just stop training when the reward reaches 200, as we did with the random search policy? That might not be a good idea. Remember that the agent is making continuous improvements in hill climbing. Even if it finds a weight that generates the maximum reward, it can still search around this weight for the optimal point. Here, we define the optimal policy as the one that can solve the CartPole problem. According to the following wiki page, https:/​/​github.​com/​openai/​gym/​wiki/​CartPole-​v0, \"solved\" means the average reward over 100 consecutive episodes is no less than 195."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjD4Fu8DfbqZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "1357d2f3-472c-4d6d-a64b-e61a10f45185"
      },
      "source": [
        "noise_scale = 0.01\n",
        "best_total_reward = 0\n",
        "total_rewards = []\n",
        "\n",
        "for episode in range(n_episode):\n",
        "    weight = best_weight + noise_scale * torch.rand(n_state, n_action)\n",
        "    total_reward = run_episode(env, weight)\n",
        "    if total_reward >= best_total_reward:\n",
        "        best_total_reward = total_reward\n",
        "        best_weight = weight\n",
        "        noise_scale = max(noise_scale / 2, 1e-4)\n",
        "    else:\n",
        "        noise_scale = min(noise_scale * 2, 2)\n",
        "    print('Episode {}: {}'.format(episode + 1, total_reward))\n",
        "    total_rewards.append(total_reward)\n",
        "    if episode >= 99 and sum(total_rewards[-100:]) >= 19500:\n",
        "        break"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f333f600a747>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_weight\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise_scale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtotal_reward\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mbest_total_reward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'best_weight' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYLUom3KgqXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}